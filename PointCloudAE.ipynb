{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import utils\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import torch\n",
    "import model\n",
    "import torch.optim as optim\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32  #ミニバッチのサイズ\n",
    "output_folder = "path"  #学習の結果を保存するフォルダの作成\n",
    "save_results = True #output_folderに学習の結果を保存する\n",
    "use_GPU = True #GPUを使う\n",
    "latent_size = 128 #オートエンコーダのmodelの中間表現(ボトルネック)のサイズの指定．今回は128次元に圧縮する．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 2048, 3)\n",
      "2048\n"
     ]
    }
   ],
   "source": [
    "from Dataloaders import GetDataLoaders\n",
    "\n",
    "pc_array = np.load("path")  #numpyファイルからデータセットをロードし，pc_arrayに格納\n",
    "print(pc_array.shape)  #pc_arrayの形状の確認\n",
    "\n",
    "# numpy配列からデータセットをロードし、90%～10%をランダムに訓練セットとテストセットに分ける(train=0.9, test=0.1)\n",
    "train_loader, test_loader = GetDataLoaders(npArray=pc_array, batch_size=batch_size)\n",
    "\n",
    "# Assuming all models have the same size, get the point size from the first model\n",
    "point_size = len(train_loader.dataset[0])  #データセット内の最初のサンプルを使用して、点の数を取得する．なお，今回のデータセットはすべて4096点に統一されている．\n",
    "print(point_size)  #point_sizeを表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.pyのPointCloudAEをインスタンス化し，point_sizeとlatent_sizeを引数として渡す．\n",
    "#このコードにより，netはPointCloudAEモデルのインスタンスとなり，エンコード，デコード，順伝播が行えるようになる．\n",
    "net = model.PointCloudAE(point_size,latent_size)\n",
    "\n",
    "#GPUが利用可能な場合はGPUを使うようにする\n",
    "if(use_GPU):\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    if torch.cuda.device_count() > 1: # if there are multiple GPUs use all\n",
    "        net = torch.nn.DataParallel(net)\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "#toメソッドを使用し，機械学習モデル\"net\"を，選択したデバイス(GPUまたはCPU)に転送する．\n",
    "net = net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch3d.loss import chamfer_distance # Pytorch3dライブラリからchanfer損失関数をインポート．https://pytorch3d.readthedocs.io/en/latest/modules/loss.html\n",
    "\n",
    "#最適化 : 関数の値を最小化するようにパラメータの値を求める手法>損失関数の値をできるだけ小さくするパラメータを見つける．\n",
    "#モデルのパラメータ(netのパラメータ)を最適化するために，Adamという手法を用いる．\n",
    "#Adamは確率的勾配降下法（SGD）の一種で、モデルのパラメータを更新してトレーニング中に損失を最小化する．lrパラメータは学習率を設定し，勾配のステップサイズを制御する．\n",
    "# この学習率はハイパーパラメータであり，トレーニングの効率や収束に影響を与える．\n",
    "\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch():  #訓練データ全体を1回処理するステップを定義\n",
    "    epoch_loss = 0  #エポック全体の累積損失を格納する変数epoch_lossの初期化\n",
    "    for i, data in enumerate(train_loader):  #訓練データをミニバッチ単位で反復処理するデータローダー. i は現在のミニバッチのインデックスを，dataは現在のミニバッチのデータを含むテンソル\n",
    "        optimizer.zero_grad()  #勾配の期期化．勾配はモデルのパラメータに対する損失の微分であり，ここでは勾配をゼロに設定して新しい勾配を計算する準備をする\n",
    "        \n",
    "        data = data.to(device)  #ミニバッチ内のデータを計算に使用する前に，データをGPUに．移動する．\n",
    "        output = net(data.permute(0,2,1))  #データの形状を(データ数，点の数，次元(3))にする．\n",
    "        loss, _ = chamfer_distance(data, output)  #chamfer損失関数を使い，ミニバッチ内のデータとモデルの出力との間の損失(誤差)を計算する．\n",
    "        loss.backward()  #誤差逆伝播を実行し，各パラメータに対する損失の勾配を計算する．\n",
    "        optimizer.step()  #オプティマイザを使用して，モデルのパラメータを計算した勾配に基づいて更新する．モデルのパラメータは訓練プロセスの中で調整され，損失を最小化するように向かう．\n",
    "        \n",
    "        epoch_loss += loss.item()  #現在のミニバッチの損失をepoch_lossに累積する．これにより，エポック全体の損失が計算される．\n",
    "        \n",
    "    return epoch_loss/i  #エポックの終了時に，エポック全体の損失をミニバッチ数で割って平均化し，その平均損失を返す．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_batch(data):  #この関数は，機械学習モデルを使用してテストデータのバッチに対して評価を行うための関数．テストデータのバッチを受け取り，評価結果を返す．\n",
    "    with torch.no_grad():  #torch.no_grad()ブロック内では，勾配情報が保持されず，計算グラフの追跡が無効になる．これにより，モデルのパラメータは更新されない．テスト時には通常，勾配情報は不要．これにより，メモリの節約や計算速度の向上ができる．\n",
    "        data = data.to(device)  #テストデータバッチをGPUに移動．\n",
    "        output = net(data.permute(0,2,1))  #データの形状を(データ数，点の数，次元(3))にする．\n",
    "        loss, _ = chamfer_distance(data, output)  #chamfer損失関数を使い，テストデータバッチとモデルの出力との間の損失(誤差)を計算する．\n",
    "        \n",
    "    return loss.item(), output.cpu()  #計算された損失とモデルの出力を返す．loss.item()は損失の値を取得し，output.cpu()はモデルの出力をcpu上に移動して返す．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def test_epoch(): # test with all test set\n",
    "#     with torch.no_grad():\n",
    "#         epoch_loss = 0\n",
    "#         for i, data in enumerate(test_loader):\n",
    "#             loss, output = test_batch(data)\n",
    "#             epoch_loss += loss\n",
    "\n",
    "#     return epoch_loss/i\n",
    "\n",
    "\n",
    "#修正を加えた以下のコードを使用．\n",
    "def test_epoch():  #テストデータセット全体に対する評価を行うための関数であるtest_epochを定義．この関数は，テストデータセットを用いてモデルを評価し，エポック全体の損失を計算する．\n",
    "    with torch.no_grad():  #torch.no_grad()ブロック内では，勾配情報が保持されず，計算グラフの追跡が無効になる．これにより，モデルのパラメータは更新されない．テスト時には通常，勾配情報は不要．これにより，メモリの節約や計算速度の向上ができる．\n",
    "        epoch_loss = 0  #エポック全体の累積損失を格納する変数epoch_lossの初期化\n",
    "        num_batches = len(test_loader)  #テストデータセット内のミニバッチ数を取得する．これはテストデータをミニバッチ単位で評価するために使用される．\n",
    "        for i, data in enumerate(test_loader):  #テストデータをミニバッチ単位で反復処理するデータローダー. i は現在のミニバッチのインデックスを，dataは現在のミニバッチのデータを含むテンソル\n",
    "            loss, output = test_batch(data)  #test_batch関数を呼び出して，現在のミニバッチに対する損失とモデルの出力を取得する．lossには現在のミニバッチの損失が，outputにはモデルの出力が格納される．\n",
    "            epoch_loss += loss  #現在のミニバッチの損失をepoch_lossに累積する．これにより，エポック全体の損失が計算される．\n",
    "\n",
    "    return epoch_loss / num_batches  #エポックの終了時に，エポック全体の損失をミニバッチ数で割って平均化し，その平均損失を返す．これは，エポックごとのテスト損失の評価値となる．モデルの性能を示す重要な指標である．\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save_resultsが真の場合に，結果を保存する前に指定した output_folder の中身をクリア（削除）する\n",
    "#データやファイルをクリアすることで，古い結果が新しい結果に上書きされることなく，フォルダ内に新しい結果を保存できる\n",
    "if(save_results):\n",
    "    utils.clear_folder(output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to output_100data_2048points/model_100data_2048points.pth\n"
     ]
    }
   ],
   "source": [
    "# ベストモデルのテストロスとモデルを保存するための変数を初期化\n",
    "best_test_loss = float('inf')  # ベストモデルの初期テストロスを正の無限大で初期化\n",
    "best_model_path = None  # ベストモデルの保存パスを初期化\n",
    "#上_20231011追加\n",
    "\n",
    "#エポックごとの訓練およびテスト損失を保存するためのリスト．最初は空のリストとして保存される．\n",
    "train_loss_list = []  \n",
    "test_loss_list = []  \n",
    "\n",
    "#エポックごとに訓練とテストを実行\n",
    "for i in range(201) :\n",
    "\n",
    "    startTime = time.time()  #各エポックの開始時にstartTimeを記録し，訓練とテストの損失を計算する．\n",
    "    \n",
    "    #train_epoch 関数を呼び出して訓練を行い，そのエポックの訓練損失を計算し，train_loss_list に追加\n",
    "    train_loss = train_epoch() #1エポックを訓練し，平均損失を得る\n",
    "    train_loss_list.append(train_loss)\n",
    "    \n",
    "    #test_epoch 関数を呼び出してテストを行い，そのエポックのテスト損失を計算し，test_loss_list に追加\n",
    "    test_loss = test_epoch() # テスト用セットによるテスト\n",
    "    test_loss_list.append(test_loss)\n",
    "    \n",
    "    epoch_time = time.time() - startTime  #エポックの実行にかかる時間を計算\n",
    "    \n",
    "    #エポックごとに、訓練とテストの損失、およびエポックの実行時間を文字列として writeString に格納\n",
    "    writeString = \"epoch \" + str(i) + \" train loss : \" + str(train_loss) + \" test loss : \" + str(test_loss) + \" epoch time : \" + str(epoch_time) + \"\\n\"\n",
    "    \n",
    "    #train_loss_list と test_loss_list のデータを使用し，訓練およびテスト損失のグラフをプロット\n",
    "    plt.plot(train_loss_list, label=\"Train\")\n",
    "    plt.plot(test_loss_list, label=\"Test\")\n",
    "    plt.legend()\n",
    "\n",
    "    if(save_results): # save_results が True の場合、結果を保存するための処理を行う．\n",
    "\n",
    "        # テキストファイル \"prints.txt\" に writeString の内容を追記する．これにより、エポックごとの訓練およびテスト結果がテキストファイルに保存される．\n",
    "        with open(output_folder + \"prints.txt\",\"a\") as file: \n",
    "            file.write(writeString)\n",
    "\n",
    "        # 訓練およびテスト損失のグラフを画像ファイル \"loss.png\" として保存．\n",
    "        plt.savefig(output_folder + \"loss.png\")\n",
    "        plt.close()\n",
    "\n",
    "        # i (エポック数)の値が50の倍数の場合、テストデータ test_samples を使って結果のプロットと保存を行う．\n",
    "        # テストデータとモデルの出力を画像ファイルとして保存し、また Numpy の npy ファイルとして test_samples を保存する．\n",
    "        if(i%50==0):\n",
    "            test_samples = next(iter(test_loader))\n",
    "            loss , test_output = test_batch(test_samples)\n",
    "            utils.plotPCbatch(test_samples, test_output, show=False, save=True, name = (output_folder  + \"epoch_\" + str(i)))\n",
    "\n",
    "            # ここでtest_samplesをNumpyのnpyファイルとして保存\n",
    "            np.save(output_folder + \"test_samples_epoch_\" + str(i) + \".npy\", test_samples)\n",
    "            np.save(output_folder + \"model_output_epoch_\" + str(i) + \".npy\", test_output)\n",
    "\n",
    "\n",
    "    else : # save_results が False の場合、結果を表示するだけの処理を行う．今回はsaveするので，この処理は行われない．\n",
    "        \n",
    "        test_samples = next(iter(test_loader))\n",
    "        loss , test_output = test_batch(test_samples)\n",
    "        utils.plotPCbatch(test_samples,test_output)\n",
    "\n",
    "        print(writeString)\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "#20231011追加\n",
    "     # ベストモデルの保存\n",
    "    if test_loss < best_test_loss:\n",
    "        best_test_loss = test_loss  # ベストテストロスを更新\n",
    "        best_model_path = os.path.join(output_folder, 'best_model.pth')  # ベストモデルの保存パスを指定\n",
    "\n",
    "        # ベストモデルを保存\n",
    "        torch.save(net.state_dict(), best_model_path)\n",
    "\n",
    "# 最終的にベストモデルを保存\n",
    "if best_model_path:\n",
    "    os.rename(best_model_path, os.path.join(output_folder, 'path'))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# モデルの保存先とファイル名を指定\n",
    "model_save_path = os.path.join(output_folder, 'path')\n",
    "\n",
    "# モデルの状態を保存\n",
    "torch.save(net.state_dict(), model_save_path)\n",
    "\n",
    "# モデルの状態が保存されました\n",
    "print(f'Model saved to {model_save_path}')\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
